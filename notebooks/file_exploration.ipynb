{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U pandas lxml networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import NavigableString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jovyan/shared/C_amc_141/R_amc_3.1_12921/203_vert_spacy_rftt/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = pd.read_pickle('../outputs/df_files.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>name</th>\n",
       "      <th>extension</th>\n",
       "      <th>size</th>\n",
       "      <th>atime</th>\n",
       "      <th>mtime</th>\n",
       "      <th>ctime</th>\n",
       "      <th>folder</th>\n",
       "      <th>depth</th>\n",
       "      <th>parent</th>\n",
       "      <th>uid</th>\n",
       "      <th>main</th>\n",
       "      <th>source</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artikel_/APA/1986/APA_19860220.xml</td>\n",
       "      <td>APA_19860220</td>\n",
       "      <td>xml</td>\n",
       "      <td>6142251</td>\n",
       "      <td>2020-02-13 14:35:32</td>\n",
       "      <td>2019-09-20 08:04:05</td>\n",
       "      <td>2019-09-23 11:43:22</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1005</td>\n",
       "      <td>artikel_</td>\n",
       "      <td>APA</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-02-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>artikel_/APA/1986/APA_19860102.xml</td>\n",
       "      <td>APA_19860102</td>\n",
       "      <td>xml</td>\n",
       "      <td>3397140</td>\n",
       "      <td>2020-02-13 14:35:33</td>\n",
       "      <td>2019-09-20 08:04:05</td>\n",
       "      <td>2019-09-23 11:43:22</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1005</td>\n",
       "      <td>artikel_</td>\n",
       "      <td>APA</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artikel_/APA/1986/APA_19860222.xml</td>\n",
       "      <td>APA_19860222</td>\n",
       "      <td>xml</td>\n",
       "      <td>3352934</td>\n",
       "      <td>2020-02-13 14:35:33</td>\n",
       "      <td>2019-09-20 08:04:05</td>\n",
       "      <td>2019-09-23 11:43:22</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1005</td>\n",
       "      <td>artikel_</td>\n",
       "      <td>APA</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-02-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>artikel_/APA/1986/APA_19860103.xml</td>\n",
       "      <td>APA_19860103</td>\n",
       "      <td>xml</td>\n",
       "      <td>3794819</td>\n",
       "      <td>2020-02-13 14:35:33</td>\n",
       "      <td>2019-09-20 08:04:06</td>\n",
       "      <td>2019-09-23 11:43:22</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1005</td>\n",
       "      <td>artikel_</td>\n",
       "      <td>APA</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>artikel_/APA/1986/APA_19860221.xml</td>\n",
       "      <td>APA_19860221</td>\n",
       "      <td>xml</td>\n",
       "      <td>5763730</td>\n",
       "      <td>2020-02-13 14:35:33</td>\n",
       "      <td>2019-09-20 08:04:06</td>\n",
       "      <td>2019-09-23 11:43:22</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1005</td>\n",
       "      <td>artikel_</td>\n",
       "      <td>APA</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-02-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 path          name extension     size  \\\n",
       "0  artikel_/APA/1986/APA_19860220.xml  APA_19860220       xml  6142251   \n",
       "1  artikel_/APA/1986/APA_19860102.xml  APA_19860102       xml  3397140   \n",
       "2  artikel_/APA/1986/APA_19860222.xml  APA_19860222       xml  3352934   \n",
       "3  artikel_/APA/1986/APA_19860103.xml  APA_19860103       xml  3794819   \n",
       "4  artikel_/APA/1986/APA_19860221.xml  APA_19860221       xml  5763730   \n",
       "\n",
       "                 atime                mtime                ctime  folder  \\\n",
       "0  2020-02-13 14:35:32  2019-09-20 08:04:05  2019-09-23 11:43:22   False   \n",
       "1  2020-02-13 14:35:33  2019-09-20 08:04:05  2019-09-23 11:43:22   False   \n",
       "2  2020-02-13 14:35:33  2019-09-20 08:04:05  2019-09-23 11:43:22   False   \n",
       "3  2020-02-13 14:35:33  2019-09-20 08:04:06  2019-09-23 11:43:22   False   \n",
       "4  2020-02-13 14:35:33  2019-09-20 08:04:06  2019-09-23 11:43:22   False   \n",
       "\n",
       "   depth  parent   uid      main source  year       date  \n",
       "0      4       5  1005  artikel_    APA  1986 1986-02-20  \n",
       "1      4       5  1005  artikel_    APA  1986 1986-01-02  \n",
       "2      4       5  1005  artikel_    APA  1986 1986-02-22  \n",
       "3      4       5  1005  artikel_    APA  1986 1986-01-03  \n",
       "4      4       5  1005  artikel_    APA  1986 1986-02-21  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining a single file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shared/C_amc_141/R_amc_3.1_12921/203_vert_spacy_rftt/artikel_/APA/1986/APA_19860220.xml\n"
     ]
    }
   ],
   "source": [
    "filepath = os.path.join(path, df_files.loc[0, 'path'])\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head -20 /home/jovyan/shared/C_amc_141/R_amc_3.1_12921/203_vert_spacy_rftt/artikel_/APA/1986/APA_19860220.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, \"r\") as f:\n",
    "    soup = bs(f, \"lxml-xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags,strings = spacify_soup(soup.file)\n",
    "#tags, strings, spaces = spacify_token_soup(soup.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<file>\\n<doc autor=\"wm\" region=\"agesamt\" datum=\"1986-02-20\" ressort2=\"ausland chronik\" tokens=\"292\" keys=\"JA_1986 AG_APA RS_CA RS_C DA_19860220 MO_198602 DB_APA\" id=\"APA_19860220_APA0001\" mediatype=\"print\" datum_full=\"1986-02-20T00:00:00Z\" docsrc=\"APA\" docsrc_name=\"APA-Meldungen digital\" bibl=\"APA-Meldungen digital  vom 1986-02-20\"',\n",
       " '>\\n<field name=\"stichwort\">\\n<s>\\n',\n",
       " '</s>\\n</field>\\n<field name=\"stichwort\">\\n<s>\\n']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 \t ['Umwelt', 'umwelt', '0', 'NOUN', 'NN', '', 'O', '0', 'ROOT', '0', '-', 'Umwelt', 'N.Reg.Nom.Sg.Fem', 'NN', 'Umwelt-n', 'tt\\n']\n",
      "16 \t ['USA', 'usa', '0', 'PROPN', 'NE', 'LOC', 'B', '0', 'ROOT', '0', '-', '-', 'N.Name.Gen.Sg.*', 'NE', 'USA-n', 'tt\\n']\n",
      "16 \t ['F', 'f', '0', 'X', 'FM', '', 'O', '-', 'ROOT', '0', '-', '-', 'N.Reg.Nom.Sg.Neut', 'NN', 'F-n', 'tt\\ne']\n",
      "16 \t ['Geht', 'geht', '0', 'VERB', 'VVFIN', '', 'O', '-', 'ROOT', '0', '-', 'gehen', 'VFIN.Full.3.Sg.Pres.Ind', 'VVFIN', 'gehen-v', 'tt\\nAmerika']\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    s = strings[i].split('\\t')[0:16]\n",
    "    print(len(s),'\\t', strings[i].split('\\t')[0:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<file>\n",
      "<doc autor=\"wm\" region=\"agesamt\" datum=\"1986-02-20\" ressort2=\"ausland chronik\" tokens=\"292\" keys=\"JA_1986 AG_APA RS_CA RS_C DA_19860220 MO_198602 DB_APA\" id=\"APA_19860220_APA0001\" mediatype=\"print\" datum_full=\"1986-02-20T00:00:00Z\" docsrc=\"APA\" docsrc_name=\"APA-Meldungen digital\" bibl=\"APA-Meldungen digital  vom 1986-02-20\">\n",
      "<field name=\"stichwort\">\n",
      "<s>\n",
      "Umwelt\tumwelt\t0\tNOUN\tNN\t\tO\t0\tROOT\t0\t-\tUmwelt\tN.Reg.Nom.Sg.Fem\tNN\tUmwelt-n\ttt\n",
      "</s>\n",
      "</field>\n",
      "<field name=\"stichwort\">\n",
      "<s>\n",
      "USA\tusa\t0\tPROPN\tNE\tLOC\tB\t0\tROOT\t0\t-\t-\tN.Name.Gen.Sg.*\tNE\tUSA-n\ttt\n",
      "</s>\n",
      "</field>\n",
      "<field name=\"stichwort\">\n",
      "<s>\n",
      "F\tf\t0\tX\tFM\t\tO\t-\tROOT\t0\t-\t-\tN.Reg.Nom.Sg.Neut\tNN\tF-n\ttt\n",
      "e\te\t1\tX\tFM\t\tO\t-\tuc\t0\t-\t-\tFM\tADJA\te-n\tu\n",
      "a\ta\t2\tX\tFM\t\tO\t-\tuc\t0\t-\t-\tFM\tFM\ta-x\ttt\n",
      "t\tt\t3\tX\tFM\t\tO\t-\tuc\t0\t-\t-\tFM\tNN\tt-n\ttt\n",
      "u\tu\t4\tX\tFM\t\tO\t-\tuc\t0\t-\t-\tN.Name.Nom.Sg.Neut\tADJA\tU-n\td\n",
      "r\tr\t5\tX\tFM\t\tO\t-\tuc\t0\t-\t-\tN.Name.Nom.Sg.Neut\tADJA\tR-n\td\n",
      "e\te\t6\tX\tFM\t\tO\t-\tuc\t0\t-\t-\tN.Name.*.*.*\tNN\tE-n\ttt\n",
      "</s>\n",
      "</field>\n",
      "<field name=\"titel\">\n",
      "<p>\n",
      "<s>\n",
      "Geht\tgeht\t0\tVERB\tVVFIN\t\tO\t-\tROOT\t0\t-\t\n"
     ]
    }
   ],
   "source": [
    "text = combine_tags_strings(tags, strings)\n",
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_reptok(fn, ln, short=\"\", titel=\"\"):\n",
    "    lns = ln.split()\n",
    "    prev_patt = \"[^a-zA-ZüäöÜÄÖß0-9]\"#\"\\s*\"\n",
    "    first_patt = \"^\"#\"\\s*\"\n",
    "\n",
    "    if(len(lns) > 2):\n",
    "        rep = \"\"\n",
    "        ln = \"\"\n",
    "        for w in lns:\n",
    "            ln += w + \"\\s*\"\n",
    "            rep += w\n",
    "        ln = \"\\s*\" + ln + \"(e)?(s)?\\s*\"\n",
    "        rep = rep[0].upper() + rep[1:].lower()\n",
    "    else:\n",
    "        rep = ln[0].upper() + ln[1:].lower()\n",
    "        ln = \"\\s*\" + ln + \"(e)?(s)?\\s*\"\n",
    "    \n",
    "    rep_str = prev_patt + fn + ln + \"|\"\n",
    "    rep_str += first_patt + fn + ln + \"|\"\n",
    "    rep_str += prev_patt + fn[0] + \".\" + ln + \"|\"\n",
    "    rep_str += first_patt + fn[0] + \".\" + ln + \"|\"\n",
    "    rep_str += prev_patt + ln[3:] + \"|\"\n",
    "    rep_str += first_patt + ln[3:]\n",
    "\n",
    "    if(short):\n",
    "        rep_str += \"|\" + prev_patt + short + \"\\s*\"\n",
    "        rep_str += \"|\" + first_patt + short + \"\\s*\"\n",
    "    if(titel):\n",
    "        rep_str += \"|\" + prev_patt + titel + \"\\s*\" + fn + ln + \"|\"\n",
    "        rep_str +=  first_patt + titel + \"\\s*\" + fn + ln + \"|\"\n",
    "        rep_str +=  prev_patt + titel + \"\\s*\" + fn[0] + \".\" + ln + \"|\"\n",
    "        rep_str +=  first_patt + titel + \"\\s*\" + fn[0] + \".\" + ln + \"|\"\n",
    "        rep_str +=  prev_patt + titel + ln + \"|\"\n",
    "        rep_str +=  first_patt + titel + ln\n",
    "    \n",
    "    return (\" \" + rep + \" \", re.compile(rep_str, flags=re.MULTILINE|re.DOTALL|re.IGNORECASE))\n",
    "\n",
    "pParagraph = r'<p>(.*?)</p>'\n",
    "pReplaceToken = [gen_reptok(\"Alexander\", \"Van der Bellen\", \"vdb\", \"Dr.\"),\n",
    "                 gen_reptok(\"Norbert\", \"Hofer\", titel=\"Ing.\"),\n",
    "                 gen_reptok(\"Irmgard\", \"Griss\", titel=\"Dr.\"),\n",
    "                 gen_reptok(\"Rudolf\", \"Hundstorfer\"),\n",
    "                 gen_reptok(\"Andreas\", \"Kohl\", titel=\"Dr.\"),\n",
    "                 gen_reptok(\"Richard\", \"Lugner\", titel=\"Ing.\")]\n",
    "pWhitesp = r\"\\s+\"\n",
    "\n",
    "pDocs = r'<doc ([^>]*?)>(.*?)</doc>'\n",
    "pMetadata = r'([^=\\s]*?)=\"([^\"]*?)\"'\n",
    "\n",
    "def get_tags_strings(splitIndex, it):\n",
    "    ret = []\n",
    "    for in_file in it:\n",
    "        soup = bs(in_file , \"lxml-xml\")\n",
    "        ret.append(spacify_soup(soup.file))\n",
    "    return ret\n",
    "\n",
    "def combine_tags_strings(tags, strings):\n",
    "    return \"\".join([tags[i] + strings[i] for i in range(len(strings))]) + tags[-1]\n",
    "\n",
    "def spacify_soup(parent, tags=[\"\"], strings=[]):\n",
    "    tags[-1] += \"<\" + parent.name\n",
    "    if(parent.attrs):\n",
    "         tags[-1] += \" \" + \" \".join([(k + '=\"' + a + '\"') for k,a in parent.attrs.items()])\n",
    "    if(parent.name == \"doc\"):\n",
    "        strings.append(\"\")\n",
    "        tags.append(\"\")\n",
    "    tags[-1] += \">\\n\"\n",
    "\n",
    "    for d in parent.children:\n",
    "        if(isinstance(d, NavigableString)):\n",
    "            if(str.strip(d)):\n",
    "                strings.append(str.strip(d)  + \"\\n\")\n",
    "                tags.append(\"\")            \n",
    "        else:\n",
    "            spacify_soup(d, tags, strings)\n",
    "    tags[-1] += \"</\" + parent.name + \">\\n\"\n",
    "    return (tags, strings)\n",
    "\n",
    "def spacify_token_soup(parent, tags=[\"\"], strings=[], spaces=[]):\n",
    "    if((not parent.name in [\"to-be-deleted-by-tree-tagger\"])):\n",
    "        tags[-1] += \"<\" + parent.name\n",
    "        if(parent.attrs):\n",
    "             tags[-1] += \" \" + \" \".join([(k + '=\"' + a + '\"') for k,a in parent.attrs.items()])\n",
    "        if(not parent.name == \"doc\"):\n",
    "            tags[-1] += \">\\n\"\n",
    "\n",
    "        if(parent.name in [\"doc\"]):\n",
    "            strings.append([])\n",
    "            spaces.append([])\n",
    "            tags.append(\"\")\n",
    "\n",
    "        if(parent.name in [\"field\", \"meta_info\", \"p\", \"s\", \"fmt\", \"section\", \"person\", \"rb\", \"comment\", \"timestamp\", \"pb\", \"cell\", \"desc\", \"div\", \"doc\", \"emph\", \"head\", \"incident\", \"item\", \"label\", \"list\", \"milestone\", \"name\", \"note\", \"p\", \"quote\", \"row\", \"seg\", \"table\", \"time\", \"u\"] and \\\n",
    "           [d for d in parent.children if isinstance(d, NavigableString) and str.strip(d)]):\n",
    "            strings.append([])\n",
    "            spaces.append([])\n",
    "            tags.append(\"\")\n",
    "\n",
    "        if(parent.name == \"doc\"):\n",
    "            #print(\"doc\", parent.attrs[\"id\"], flush=True)\n",
    "            tags[-1] += \">\\n\"\n",
    "           \n",
    "    for d in parent.children:\n",
    "        if(isinstance(d, NavigableString)):\n",
    "            if(str.strip(d)):\n",
    "                words = [w for w in d.split(\"\\n\") if str.strip(w)]\n",
    "                strings[-1].extend(words)\n",
    "                spaces[-1].extend([True for _ in range(len(words))])\n",
    "        elif (d.name == \"g\" and spaces[-1]):\n",
    "            #print(parent.name, d.name, flush=True)\n",
    "            spaces[-1][-1] = False\n",
    "        else:\n",
    "            spacify_token_soup(d, tags, strings, spaces)\n",
    "    if((not parent.name in [\"to-be-deleted-by-tree-tagger\"])):\n",
    "        tags[-1] += \"</\" + parent.name + \">\\n\"\n",
    "\n",
    "    return (tags, strings, spaces)\n",
    "\n",
    "def get_paragraphs_filtered_file(in_file, metadata_filter):\n",
    "    (p, text) = in_file\n",
    "    ret = []\n",
    "    for (repl, patt) in pReplaceToken:\n",
    "        text = re.sub(patt, repl, text)\n",
    "\n",
    "    for (metadata, content) in re.findall(pDocs, text, flags=re.MULTILINE|re.DOTALL):\n",
    "        metadata = re.findall(pMetadata, metadata, flags=re.MULTILINE|re.DOTALL)\n",
    "        if(any(lambda mf: mf in [el for x in metadata for el in x], metadata_filter)):\n",
    "            content = \" \".join(re.findall(pParagraph, content.lower(), flags=re.MULTILINE|re.DOTALL))\n",
    "            ret.append(content)\n",
    "    return ret \n",
    "    #return re.findall(pParagraph, text, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "def get_paragraphs_filtered(splitIndex, it, metadata_filter):\n",
    "    ret = []\n",
    "    for in_file in it:\n",
    "        ret.extend(get_paragraphs_filtered_file(in_file, metadata_filter))\n",
    "    return ret\n",
    "\n",
    "def get_paragraphs_file(in_file):\n",
    "    (p, text) = in_file\n",
    "    ret = []\n",
    "    for (repl, patt) in pReplaceToken:\n",
    "        text = re.sub(patt, repl, text)\n",
    "    for (metadata, content) in re.findall(pDocs, text, flags=re.MULTILINE|re.DOTALL):\n",
    "        #metadata = re.findall(pMetadata, metadata, flags=re.MULTILINE|re.DOTALL)\n",
    "        content = \" \".join(re.findall(pParagraph, content.lower(), flags=re.MULTILINE|re.DOTALL))\n",
    "        ret.append(content)\n",
    "    return ret \n",
    "    #return re.findall(pParagraph, text, re.MULTILINE|re.DOTALL)\n",
    "\n",
    "def get_docs(splitIndex, it):\n",
    "    ret = []\n",
    "    for in_file in it:\n",
    "        ret.append(get_paragraphs_file(in_file))\n",
    "    return ret\n",
    "\n",
    "def get_paragraph_file(in_file):\n",
    "    (p, text) = in_file\n",
    "    ret = []\n",
    "    for (repl, patt) in pReplaceToken:\n",
    "        text = re.sub(patt, repl, text)# flags=re.MULTILINE|re.DOTALL|re.IGNORECASE)\n",
    "    for (metadata, content) in re.findall(pDocs, text, flags=re.MULTILINE|re.DOTALL):\n",
    "        metadata = re.findall(pMetadata, metadata, flags=re.MULTILINE|re.DOTALL)\n",
    "        content = re.findall(pParagraph, content.lower(), flags=re.MULTILINE|re.DOTALL)\n",
    "        ret.extend(content)\n",
    "\n",
    "    return ret \n",
    "    #return re.findall(pParagraph, text, re.MULTILINE|re.DOTALL)\n",
    "\n",
    "def get_paragraph(splitIndex, it):\n",
    "    ret = []\n",
    "    for in_file in it:\n",
    "        ret.extend(get_paragraph_file(in_file))\n",
    "        \n",
    "    return ret\n",
    "\n",
    "def get_paragraphs(splitIndex, it):\n",
    "    ret = []\n",
    "    for in_file in it:\n",
    "        ret.extend(get_paragraphs_file(in_file))\n",
    "        \n",
    "    return ret\n",
    "\n",
    "def get_paragraphs_metadata_zipped2_file(in_file):\n",
    "    (p, text) = in_file\n",
    "    ret = []\n",
    "    for (repl, patt) in pReplaceToken:\n",
    "        text = re.sub(patt, repl, text)\n",
    "    for (metadata, content) in re.findall(pDocs, text, flags=re.MULTILINE|re.DOTALL):\n",
    "        metadata = dict(re.findall(pMetadata, metadata, flags=re.MULTILINE|re.DOTALL))\n",
    "        content = re.findall(pParagraph, content.lower(), flags=re.MULTILINE|re.DOTALL)\n",
    "        for p in content:\n",
    "            ret.append((metadata[\"id\"], p))\n",
    "    return ret\n",
    "    \n",
    "def get_paragraphs_metadata_zipped2(splitIndex, it):\n",
    "    ret = []\n",
    "    for in_file in it:\n",
    "        ret.extend(get_paragraphs_metadata_zipped2_file(in_file))\n",
    "    return ret\n",
    "\n",
    "def get_paragraphs_metadata_zipped_file(in_file):\n",
    "    (p, text) = in_file\n",
    "    ret = []\n",
    "    for (repl, patt) in pReplaceToken:\n",
    "        text = re.sub(patt, repl, text)\n",
    "    for (metadata, content) in re.findall(pDocs, text, flags=re.MULTILINE|re.DOTALL):\n",
    "        dmeta = dict(re.findall(pMetadata, metadata, flags=re.MULTILINE|re.DOTALL))\n",
    "        content = re.findall(pParagraph, content, flags=re.MULTILINE|re.DOTALL)\n",
    "        #smeta = dmeta[\"id\"] + \";\" + dmeta[\"datum\"] + \";\" + dmeta[\"docsrc\"] + \";\" + dmeta[\"ressort2\"] + \".\"\n",
    "        smeta = [dmeta[\"id\"], dmeta[\"datum\"], dmeta[\"docsrc\"], dmeta[\"ressort2\"]]\n",
    " \n",
    "        for p in content:\n",
    "            #p = re.sub('[^a-zA-ZüäöÜÄÖß0-9-,.:\\s]', '', p)\n",
    "            if(p and len(p.split()) > 3):\n",
    "                ret.append((smeta, p))\n",
    "                #ret.append(smeta + p)\n",
    "        #ret.append((dict(metadata)[\"id\"], metadata))\n",
    "\n",
    "    return ret\n",
    "    \n",
    "def get_paragraphs_metadata_zipped(splitIndex, it):\n",
    "    ret = []\n",
    "    for in_file in it:\n",
    "        ret.extend(get_paragraphs_metadata_zipped_file(in_file))\n",
    "        \n",
    "    return ret\n",
    "\n",
    "def get_paragraphs_metadata_file(in_file):\n",
    "    (p, text) = in_file\n",
    "    ret = []\n",
    "\n",
    "    for (repl, patt) in pReplaceToken:\n",
    "        text = re.sub(patt, repl, text)\n",
    "\n",
    "    for (metadata, content) in re.findall(pDocs, text, flags=re.MULTILINE|re.DOTALL):\n",
    "        metadata = re.findall(pMetadata, metadata, flags=re.MULTILINE|re.DOTALL)\n",
    "        #content = \" \".join(re.findall(pParagraph, content, re.MULTILINE|re.DOTALL))\n",
    "        ret.append((metadata, 0))#content))\n",
    "\n",
    "    return ret\n",
    "    \n",
    "def get_paragraphs_metadata(splitIndex, it):\n",
    "    ret = []\n",
    "    for in_file in it:\n",
    "        ret.extend(get_paragraphs_metadata_file(in_file))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def filter_sentences(l):\n",
    "    ret = []\n",
    "    for paragraph in l:\n",
    "        paragraph_list = []\n",
    "        for sentence in re.sub('[^a-zA-ZüäöÜÄÖß0-9-.\\s]', '', paragraph).split(\".\"):\n",
    "            tokenized_sentence = sentence.replace(\"\\n\", \" \").split(\" \")\n",
    "            non_empty_string_sentece = filter(None, tokenized_sentence)\n",
    "            if(non_empty_string_sentece):\n",
    "                paragraph_list.extend(non_empty_string_sentece)\n",
    "        if(paragraph_list):\n",
    "            ret.append(paragraph_list)\n",
    "    return ret\n",
    "\n",
    "def remove_stopwords(tokenized_df):\n",
    "    #Set params for StopWordsRemover\n",
    "    german_stopw = StopWordsRemover.loadDefaultStopWords(\"german\")\n",
    "    with open(\"/tmp/data/stopwords-json/dist/de.json\", \"r\") as f:\n",
    "        ger_stopwords = set([\"mehr\", \"ganz\", \"kurz\", \"macht\", \"geht\", \"list\", \n",
    "                             \"radio-tipps\", \"switch\", \"neue\", \"heute\", \"euro\", \n",
    "                             \"beim\", \"zwei\", \"gibt\", \"drei\", \"jahre\", \"neuer\", \n",
    "                             \"wurde\", \"schon\", \"zurück\", \"neues\", \"überblick\", \n",
    "                             \"the\", \"apa\", \"prozent\", \"utl\", \"österreich\", \"amp\", \n",
    "                             \"laut\", \"-jährige\", \"leben\", \"http\", \"land\", \"stadt\", \n",
    "                             \"montag\", \"dienstag\", \"mittwoch\", \"donnerstag\", \"freitag\", \n",
    "                             \"samstag\", \"sonntag\", \"derzeit\", \"wwwotsat\", \"ots\", \"geben\", \"stehen\"] + json.load(f))\n",
    "    german_stopw.extend(ger_stopwords)\n",
    "    remover = StopWordsRemover().setStopWords(german_stopw).setInputCol(\"tokens\").setOutputCol(\"filtered\")\n",
    "\n",
    "    #Create new DF with Stopwords removed\n",
    "    return remover.transform(tokenized_df)\n",
    "\n",
    "def tokenize(corpus_df):\n",
    "    tokenizer = RegexTokenizer().setPattern(\"[^a-zA-ZüäöÜÄÖß_-]+\").setMinTokenLength(3).setInputCol(\"text\").setOutputCol(\"tokens\")\n",
    "    return tokenizer.transform(corpus_df)\n",
    "\n",
    "def vectorize(filtered_df):\n",
    "    vectorizer = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").setMinDF(100).fit(filtered_df)\n",
    "    #.setVocabSize(50000)\n",
    "    return (vectorizer.vocabulary, vectorizer.transform(filtered_df))\n",
    "\n",
    "def prepare_corpus_lda(corpus):\n",
    "    corpus_df = corpus.zipWithIndex().toDF([\"text\", \"id\"])\n",
    "    tokenized_df = tokenize(corpus_df)\n",
    "    filtered_df = remove_stopwords(tokenized_df)\n",
    "    (vocabArray, vectorized_df) = vectorize(filtered_df)\n",
    "    return (vocabArray, vectorized_df.select(\"id\", \"features\"))#.cache())\n",
    "\n",
    "def tokenize_w2v(ps):\n",
    "    return [[ el.lower() for el in y.split() if len(el) > 2] \\\n",
    "            for x in map(lambda row: filter(lambda x: x != \"\", \\\n",
    "                                            re.sub('[^a-zA-ZüäöÜÄÖß0-9.\\s]', '', row).split(\".\")), ps) for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
